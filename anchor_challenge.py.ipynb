{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor Coding Challenge\n",
    "\n",
    "write a program that outputs the largest unique set of characters that can be removed from this paragraph without letting its length drop below 50. For example: [‘H’, ‘i’, ‘!’, ‘ ’]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_unique_set(text):\n",
    "    txtlength = len(text)\n",
    "    dictionary = collections.Counter(text)    \n",
    "    dictionary = sorted(dictionary.items(), key = lambda x: -x[1])\n",
    "    removed = 0\n",
    "    res = []\n",
    "    while txtlength >= 50 :\n",
    "        pair = dictionary.pop(0)\n",
    "        frequency = pair[1]\n",
    "        txtlength -= frequency\n",
    "        if txtlength >= 50:\n",
    "            res.append(pair[0])\n",
    "            removed += pair[1]\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['If you want to jumpstart the process of talking to us about this role, here’s a little challenge: write a program that outputs the largest unique set of characters that can be removed from this paragraph without letting its length drop below 50. For example: [‘H’, ‘i’, ‘!’, ‘ ’]',\n",
    "        'Move Fast: We learn by doing (that is, we’re not afraid to test our hypotheses on production), and we build things that don’t scale. We’re energized, not daunted, by the massive challenge of changing the world of podcasting. Goonies never say die!', 'Build the Future: We don’t worry about how podcasting (or even our own product) has worked in the past. We think outside the box, take huge risks, and live by the motto “Anything is possible.”', 'Creators First: We build tools that make creating audio easier, better or both. Every single feature in Anchor is something everyone can use. (And of course, the easiest way to know what helps podcasters is for us to regularly podcast ourselves.)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest list of characters that can be removed is \n",
      "[' ', 't', 'e', 'a', 'o', 'r', 's', 'h', 'l', 'i', 'u', 'p', 'n', 'g']\n",
      "The largest list of characters that can be removed is \n",
      "[' ', 'e', 't', 'o', 'n', 'a', 's', 'd', 'i', 'r', 'h', 'g']\n",
      "The largest list of characters that can be removed is \n",
      "[' ', 't', 'e', 'o', 'i', 'h', 'n', 's', 'u', 'r', 'd']\n",
      "The largest list of characters that can be removed is \n",
      "[' ', 'e', 's', 't', 'o', 'r', 'a', 'i', 'n', 'u', 'l', 'h']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for txt in texts:\n",
    "    charset = largest_unique_set(txt)\n",
    "    print('The largest list of characters that can be removed is \\n{}'.format(charset))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time and Space complexity analysis\n",
    "\n",
    "## Time complexity\n",
    "The time complexity is $O(n*log(n))$ where $n$ is the number of character in the input text.\n",
    "\n",
    "By the time the entire text was pushed into the hashmap, the time complexity is $O(n)$, but afterward, the hashmap is sorted with respect to the frequency of the key. Such sorting scheme can at worst be $O(n*log(n))$\n",
    "\n",
    "Thus, $O(n*log(n)) + O(n)$ → $O(n*log(n))$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space complexity\n",
    "The space complexity is $O(n)$, since we need extra space to save hashmap that can at most have $n$ elements, and returning set, which also can have at most $n$ elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
